{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WQIBjmVMvRq3",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f100369f1ea1e429db3e5ad4342a1d4",
     "grade": false,
     "grade_id": "cell-e5edec0425de3a50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HW Autoencoders\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will get introduced to Autoencoders, a group of architectures used for encoding compact representations of model inputs and then reconstructing them. This has a variety of real-world use cases such as compression, pre-training encoder modules, and more. It is also closely related to the Variational Autoencoder models that we will see later and which can be used to generate new synthetic data.\n",
    "\n",
    "More specifically, you will implement a vanilla and then a stacked autoencoder model. Then, you will train each on **Heart Failure Prediction** and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17 EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OaOl6bAvigPG",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "520da7c8b19531c324fc1b8fd5675b5e",
     "grade": false,
     "grade_id": "cell-e59364a32d9b5fc9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Raw Data\n",
    "\n",
    "Pneumonia is a lung disease characterized by inflammation of the airspaces in the lungs, most commonly due to an infection. In this section, you will train a CNN model to classify Pneumonia disease (Pneumonia/Normal) based on chest X-Ray images. \n",
    "\n",
    "The chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old. All chest X-ray imaging was performed as part of patientsâ€™ routine clinical care. You can refer to this [link](https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:14.326188Z",
     "start_time": "2023-02-28T02:57:13.355473Z"
    },
    "deletable": false,
    "editable": false,
    "id": "QWbsirOOvRq9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c64c6304db7ccbe2c068528004c398d2",
     "grade": false,
     "grade_id": "cell-ad33073e8b0cba4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Import all the libraries used\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "### Set random seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# record start time\n",
    "_START_RUNTIME = time.time()\n",
    "\n",
    "# Define data and weight path\n",
    "DATA_PATH = \"../HW4_Autoencoder-lib/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../HW4_Autoencoder-lib/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0be6ca11d51ca9b4a8017f78e04e0165",
     "grade": false,
     "grade_id": "cell-115765ba096f745e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Load and Visualize the Data [10 points]\n",
    "\n",
    "The data is under `DATA_PATH`. In this part, you are required to load the data into the data loader, and calculate some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:14.335735Z",
     "start_time": "2023-02-28T02:57:14.328277Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "#input\n",
    "# folder: str, 'train', 'val', or 'test'\n",
    "#output\n",
    "# number_normal: number of normal samples in the given folder\n",
    "# number_pneumonia: number of pneumonia samples in the given folder\n",
    "def get_count_metrics(folder, data_path=DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the number of normal and pneumonia samples.\n",
    "          Hint: !ls $DATA_PATH\n",
    "    '''\n",
    "    \n",
    "    normal_ls = os.listdir(os.path.join(data_path, folder, 'NORMAL'))\n",
    "    pneumonia_ls = os.listdir(os.path.join(data_path, folder, 'PNEUMONIA'))\n",
    "    return len(normal_ls), len(pneumonia_ls)\n",
    "\n",
    "\n",
    "#output\n",
    "# train_loader: train data loader (type: torch.utils.data.DataLoader)\n",
    "# val_loader: val data loader (type: torch.utils.data.DataLoader)\n",
    "def load_data(data_path=DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for \n",
    "    train and validation dataset. Set batchsize to 32.\n",
    "    \n",
    "    You should add the following transforms (https://pytorch.org/docs/stable/torchvision/transforms.html):\n",
    "        1. transforms.RandomResizedCrop: the images should be cropped to 224 x 224\n",
    "        2. transforms.RandomResizedCrop: the images should be compressed to 24 x 24\n",
    "        3. transforms.ToTensor: just to convert data/labels to tensors\n",
    "        4. flatten_transform: to flatten the images away from their 3 x 24 x 24 representation (provided)\n",
    "    You should set the *shuffle* flag for *train_loader* to be True, and False for *val_loader*.\n",
    "    \n",
    "    HINT: Consider using `torchvision.datasets.ImageFolder`.\n",
    "    '''\n",
    "\n",
    "    import torchvision\n",
    "    import torchvision.datasets as datasets\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    flatten_transform = transforms.Lambda(lambda x: torch.flatten(x))\n",
    "    transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224)),\n",
    "        transforms.RandomResizedCrop((24, 24)),\n",
    "        transforms.ToTensor(),\n",
    "        flatten_transform,\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=os.path.join(data_path, 'train'), transform=transforms)\n",
    "    val_dataset = torchvision.datasets.ImageFolder(root=os.path.join(data_path, 'val'), transform=transforms)\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:14.353331Z",
     "start_time": "2023-02-28T02:57:14.340553Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bf602ba251a32bc57b316a0dcd19d1e",
     "grade": true,
     "grade_id": "cell-458b2462e7d5c860",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert type(get_count_metrics('train')) is tuple\n",
    "assert type(get_count_metrics('val')) is tuple\n",
    "\n",
    "assert get_count_metrics('train') == (335, 387)\n",
    "assert get_count_metrics('val') == (64, 104)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:14.551861Z",
     "start_time": "2023-02-28T02:57:14.355144Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d4921243fd2f8db88a706513d42c62b",
     "grade": true,
     "grade_id": "cell-0be378369cbc90a4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "train_loader, val_loader = load_data()\n",
    "\n",
    "assert type(train_loader) is torch.utils.data.dataloader.DataLoader\n",
    "\n",
    "assert len(train_loader) == 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:16.308379Z",
     "start_time": "2023-02-28T02:57:14.553189Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a555836ab06dfbb3345ff594bb01a5b8",
     "grade": false,
     "grade_id": "cell-e908531cdc33dff4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS PART\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img, title):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def show_batch_images(dataloader, k=8):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images.reshape(-1, 3, 24, 24)\n",
    "    images = images[:k]\n",
    "    labels = labels[:k]\n",
    "    img = torchvision.utils.make_grid(images, padding=3)\n",
    "    imshow(img, title=[\"NORMAL\" if x==0  else \"PNEUMONIA\" for x in labels])\n",
    "\n",
    "train_loader, val_loader = load_data()   \n",
    "for i in range(2):\n",
    "    show_batch_images(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3vYh25HwvRq7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0289a6c9f72f08e66d0b183e6364503",
     "grade": false,
     "grade_id": "cell-25ebf45813d4a47b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4367eb6f1611b5308267396ba9388f7",
     "grade": false,
     "grade_id": "cell-b630758f18c9255c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Build the Models [30 points]\n",
    "\n",
    "In this section we will build four different variants of Autoencoder architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa522fed83638fbce9c93542b552ea10",
     "grade": false,
     "grade_id": "cell-596f4a6bda1cba51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Vanilla Autoencoder [5 points]\n",
    "\n",
    "The first thing we will do is build the simple autoencoder model. For each patient, the vanilla autoencoder model will take an input tensor of 1728-dim, and produce an output tensor of 1728-dim as well that is meant to closely mirror the original input. However, in between the model will compress those 1728 dimensions into just 16 such that it will build an intermediate representation which contains all of the information of the entire 1728 dimensions in just 16 numbers.\n",
    "\n",
    "The detailed model architecture for you to follow is shown in the table below, but it will be broken down into the encoder half and decoder half.\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "fully connected | input size 1728, output size 128 | ReLU | (32, 128)\n",
    "fully connected | input size 128, output size 16 | ReLU | (32, 16)\n",
    "fully connected | input size 16, output size 128 | ReLU | (32, 128)\n",
    "fully connected | input size 128, output size 1728 | Sigmoid | (32, 1728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:16.319632Z",
     "start_time": "2023-02-28T02:57:16.310451Z"
    },
    "deletable": false,
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the MLP shown above.\n",
    "HINT: Consider using `nn.Linear`, `torch.relu`, and `torch.sigmoid`.\n",
    "\"\"\"\n",
    "\n",
    "class VanillaAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize the model layers as shown above.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(1728, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 128)\n",
    "        self.fc4 = nn.Linear(128, 1728)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform encoding operation with fc1, fc2, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "        \n",
    "    def decode(self, x):\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))  \n",
    "\n",
    "# initialize the NN\n",
    "model = VanillaAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:16.325800Z",
     "start_time": "2023-02-28T02:57:16.321670Z"
    },
    "deletable": false,
    "editable": false,
    "id": "yiyoztOQvRrS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50204d6fa6517a1335ac969bc52da199",
     "grade": false,
     "grade_id": "cell-f76f90197c77b3d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert model.fc1.in_features == 1728, f'First layer input size is wrong! Should be 1728!={model.fc1.in_features}'\n",
    "assert model.fc1.out_features == 128, f'First layer output size is wrong! Should be 128!={model.fc1.out_features}'\n",
    "assert model.fc2.in_features == 128, f'Second layer input size is wrong! Should be 128!={model.fc2.in_features}'\n",
    "assert model.fc2.out_features == 16, f'Second layer output size is wrong! Should be 16!={model.fc2.out_features}'\n",
    "assert model.fc3.in_features == 16, f'Third layer input size is wrong! Should be 16!={model.fc3.in_features}'\n",
    "assert model.fc3.out_features == 128, f'Third layer output size is wrong! Should be 128!={model.fc3.out_features}'\n",
    "assert model.fc4.in_features == 128, f'Fourth layer input size is wrong! Should be 128!={model.fc4.in_features}'\n",
    "assert model.fc4.out_features == 1728, f'Fourth layer output size is wrong! Should be 1728!={model.fc4.out_features}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.061032Z",
     "start_time": "2023-02-28T02:57:16.328045Z"
    },
    "deletable": false,
    "editable": false,
    "id": "kssyWHVmvRrT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83a897275e9d3ebdf4d94b4f529a3d05",
     "grade": true,
     "grade_id": "cell-2856eacbec25c451",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37c38500e69fc8f012a7ae1aa3fb8afe",
     "grade": false,
     "grade_id": "cell-596f4a6bda1cba52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Sparse Autoencoder [5 Points]\n",
    "\n",
    "Next, we will be constructing a sparse autoencoder model. While the biggest difference between the Sparse Autoencoder and Vanilla Autoencoder will come later in our training function by adding regularization in the loss function, we will also use the sigmoid activation function for all of our hidden layers here as well.\n",
    "\n",
    "The detailed model architecture for you to follow is shown in the table below, and it will also be broken down into the encoder half and decoder half.\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "fully connected | input size 1728, output size 128 | Sigmoid | (32, 128)\n",
    "fully connected | input size 128, output size 16 | Sigmoid | (32, 16)\n",
    "fully connected | input size 16, output size 128 | Sigmoid | (32, 128)\n",
    "fully connected | input size 128, output size 1728 | Sigmoid | (32, 1728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.075697Z",
     "start_time": "2023-02-28T02:57:17.065689Z"
    },
    "deletable": false,
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the MLP shown above.\n",
    "HINT: Consider using `nn.Linear` and `torch.sigmoid`.\n",
    "\"\"\"\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize the model layers as shown above.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.fc1 = nn.Linear(1728, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 128)\n",
    "        self.fc4 = nn.Linear(128, 1728)\n",
    "        \n",
    "        # used in training as sparsity regularization\n",
    "        self.data_rho = 0\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform encoding operation with fc1, fc2, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "        \n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform decoding operation with fc3, fc4, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        self.data_rho = x.mean(0)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# initialize the NN\n",
    "model = SparseAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.082624Z",
     "start_time": "2023-02-28T02:57:17.077892Z"
    },
    "deletable": false,
    "editable": false,
    "id": "yiyoztOQvRrS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b2139299b8ec16db94d3f4371077e79",
     "grade": false,
     "grade_id": "cell-f76f90197c77b3d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert model.fc1.in_features == 1728, f'First layer input size is wrong! Should be 1728!={model.fc1.in_features}'\n",
    "assert model.fc1.out_features == 128, f'First layer output size is wrong! Should be 128!={model.fc1.out_features}'\n",
    "assert model.fc2.in_features == 128, f'Second layer input size is wrong! Should be 128!={model.fc2.in_features}'\n",
    "assert model.fc2.out_features == 16, f'Second layer output size is wrong! Should be 16!={model.fc2.out_features}'\n",
    "assert model.fc3.in_features == 16, f'Third layer input size is wrong! Should be 16!={model.fc3.in_features}'\n",
    "assert model.fc3.out_features == 128, f'Third layer output size is wrong! Should be 128!={model.fc3.out_features}'\n",
    "assert model.fc4.in_features == 128, f'Fourth layer input size is wrong! Should be 128!={model.fc4.in_features}'\n",
    "assert model.fc4.out_features == 1728, f'Fourth layer output size is wrong! Should be 1728!={model.fc4.out_features}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.666708Z",
     "start_time": "2023-02-28T02:57:17.084195Z"
    },
    "deletable": false,
    "editable": false,
    "id": "kssyWHVmvRrT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "415b7a22f4c25623208bf1f4aaba3fcc",
     "grade": true,
     "grade_id": "cell-2856eacbec25c452",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c3d84b4b37edb10cee624ab149e675e",
     "grade": false,
     "grade_id": "cell-596f4a6bda1cba53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Denoising Autoencoder [10 Points]\n",
    "\n",
    "Next, we will be constructing a denoising autoencoder model. This follows the Vanilla Autoencoder but adds noise to the input in order to train the model to be able to both handle noisy input as well as serve as regularization to prevent overfitting. While the input is now noisy, the model still attempts to reconstruct the original input.\n",
    "\n",
    "The detailed model architecture for you to follow is the same as with the Vanilla Autoencoder and is shown in the table below, and it will also be broken down into the encoder half and decoder half.\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "fully connected | input size 1728, output size 128 | ReLU | (32, 128)\n",
    "fully connected | input size 128, output size 16 | ReLU | (32, 16)\n",
    "fully connected | input size 16, output size 128 | ReLU | (32, 128)\n",
    "fully connected | input size 128, output size 1728 | Sigmoid | (32, 1728)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.679981Z",
     "start_time": "2023-02-28T02:57:17.668553Z"
    },
    "deletable": false,
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the MLP shown above.\n",
    "HINT: Consider using `nn.Linear`, `torch.relu`, and `torch.sigmoid`.\n",
    "\"\"\"\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize the model layers as shown above.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.fc1 = nn.Linear(1728, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 128)\n",
    "        self.fc4 = nn.Linear(128, 1728)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform encoding operation with fc1, fc2, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x\n",
    "        \n",
    "    def decode(self, x):\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = None\n",
    "        std = 0.1\n",
    "        mean = 0\n",
    "        \"\"\"\n",
    "        TODO: Generate the noise from the normal distribution with the above mean and std.\n",
    "        \n",
    "        Note that the size of the noise should be the same as x.\n",
    "        \n",
    "        Hint: Use torch.randn().\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        noise = torch.randn(x.size()) * std\n",
    "        x = x + noise\n",
    "        return self.decode(self.encode(x)) \n",
    "\n",
    "# initialize the NN\n",
    "model = DenoisingAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.687185Z",
     "start_time": "2023-02-28T02:57:17.681251Z"
    },
    "deletable": false,
    "editable": false,
    "id": "yiyoztOQvRrS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7826fb4189d1b293d9dc17878af45e2",
     "grade": false,
     "grade_id": "cell-f76f90197c77b3d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert model.fc1.in_features == 1728, f'First layer input size is wrong! Should be 1728!={model.fc1.in_features}'\n",
    "assert model.fc1.out_features == 128, f'First layer output size is wrong! Should be 128!={model.fc1.out_features}'\n",
    "assert model.fc2.in_features == 128, f'Second layer input size is wrong! Should be 128!={model.fc2.in_features}'\n",
    "assert model.fc2.out_features == 16, f'Second layer output size is wrong! Should be 16!={model.fc2.out_features}'\n",
    "assert model.fc3.in_features == 16, f'Third layer input size is wrong! Should be 16!={model.fc3.in_features}'\n",
    "assert model.fc3.out_features == 128, f'Third layer output size is wrong! Should be 128!={model.fc3.out_features}'\n",
    "assert model.fc4.in_features == 128, f'Fourth layer input size is wrong! Should be 128!={model.fc4.in_features}'\n",
    "assert model.fc4.out_features == 1728, f'Fourth layer output size is wrong! Should be 1728!={model.fc4.out_features}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:18.438717Z",
     "start_time": "2023-02-28T02:57:17.689324Z"
    },
    "deletable": false,
    "editable": false,
    "id": "kssyWHVmvRrT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbc14a43b60ac2d3a9ef6a48dc3bfd8d",
     "grade": true,
     "grade_id": "cell-2856eacbec25c453",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:19.098634Z",
     "start_time": "2023-02-28T02:57:18.439908Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c467b45d25d06c0a89e80112bfb596a8",
     "grade": true,
     "grade_id": "cell-b88a8dc040608ca3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5236f60977a812b6987988c78129c01",
     "grade": false,
     "grade_id": "cell-596f4a6bda1cba54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.4 Stacked Autoencoder [10 Points]\n",
    "\n",
    "Finally, we will be constructing a more complex and better performing stacked autoencoder model. For each patient, we will still take an input tensor of 1728-dim, and produce an output tensor of 1728-dim as well that is meant to closely mirror the original input. We will also still compress those 1728 dimensions into just 16. However, instead of performing such a compression just once, we will do it three times in a row using Vanilla Autoencoder models as subcomponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:19.117415Z",
     "start_time": "2023-02-28T02:57:19.100310Z"
    },
    "deletable": false,
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the StackedAutoencoder using your VanillaAutoencoder architecture.\n",
    "\"\"\"\n",
    "\n",
    "class StackedAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.ae1 = None\n",
    "        self.ae2 = None\n",
    "        self.ae3 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize three Vanilla Autoencoders and assign them to self.ae1, self.ae2, self.ae3, respectively.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.ae1 = VanillaAutoencoder()\n",
    "        self.ae2 = VanillaAutoencoder()\n",
    "        self.ae3 = VanillaAutoencoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ae1(x)\n",
    "        x = self.ae2(x)\n",
    "        x = self.ae3(x)\n",
    "        return x\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: While we didn't implement the forward() function of the\n",
    "        StackedAutoencoder as using an encode() and decode() function, \n",
    "        we may still be interested in the future of extracting the \n",
    "        compressed representation. So, implement the encode function\n",
    "        to return the compressed representation from the third\n",
    "        VanillaAutoencoder component (note you will have to call its \n",
    "        encode function).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        x = self.ae1(x)\n",
    "        x = self.ae2(x)\n",
    "        x = self.ae3.encode(x)\n",
    "        return x\n",
    "    \n",
    "# initialize the NN\n",
    "model = StackedAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:19.123559Z",
     "start_time": "2023-02-28T02:57:19.119488Z"
    },
    "deletable": false,
    "editable": false,
    "id": "yiyoztOQvRrS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "753629fee5ddf9d065f6157929766cd8",
     "grade": false,
     "grade_id": "cell-f76f90197c77b3d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert isinstance(model.ae1, VanillaAutoencoder), f'First autoencoder should be a VanillaAutoencoder'\n",
    "assert isinstance(model.ae2, VanillaAutoencoder), f'Second autoencoder should be a VanillaAutoencoder'\n",
    "assert isinstance(model.ae3, VanillaAutoencoder), f'Third autoencoder should be a VanillaAutoencoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:19.801270Z",
     "start_time": "2023-02-28T02:57:19.125437Z"
    },
    "deletable": false,
    "editable": false,
    "id": "kssyWHVmvRrT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5908837a5cd50b3e36ca8cfbd1f375e7",
     "grade": true,
     "grade_id": "cell-2856eacbec25c454",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.483697Z",
     "start_time": "2023-02-28T02:57:19.802959Z"
    },
    "deletable": false,
    "editable": false,
    "id": "6xfzt86LvRrT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "719460c7750ca76759d9671b5fd48626",
     "grade": true,
     "grade_id": "cell-4eb5d8c51ab28194",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "j1MoUmqmvRrU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af67e5e2e597aed2c617e7c97ace4c14",
     "grade": false,
     "grade_id": "cell-4994c16a476d76b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Training the Networks [60 points]\n",
    "\n",
    "In this step, you will train each of the three autoencoder architectures and compare the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hxfDvt8QvRrV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fa801ab896d5630e9c98747b5b2bdbd",
     "grade": false,
     "grade_id": "cell-4f20a0da1ae4d342",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Unlike most of our past loss functions that go with the classification tasks we have see, here we will be using Mean Squared Error loss which is typically used in reconstruction settings such as ours and also in regression tasks (in which outputs are numeric values instead of probabilities and class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.488662Z",
     "start_time": "2023-02-28T02:57:20.486044Z"
    },
    "deletable": false,
    "id": "TMHiaMn4vRrV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Define the loss (MSELoss), assign it to `criterion`.\n",
    "\n",
    "REFERENCE: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "\"\"\"\n",
    "\n",
    "criterion = None\n",
    "\n",
    "# your code here\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.493983Z",
     "start_time": "2023-02-28T02:57:20.490516Z"
    },
    "deletable": false,
    "editable": false,
    "id": "IRToTssVvRrV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3e74c70ca95932eac53dc8da708818f",
     "grade": false,
     "grade_id": "cell-1a0dae49f26b3ae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "_loss = criterion(torch.Tensor([0., 1., 0.5]), torch.Tensor([1., 1., 1.]))\n",
    "assert abs(_loss.tolist() - 0.4167) < 1e-3, \"MSELoss is wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.498905Z",
     "start_time": "2023-02-28T02:57:20.495633Z"
    },
    "deletable": false,
    "editable": false,
    "id": "YxFvMxyLvRrW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9831c5e5b6d16f34b9d1ce86cfb405ed",
     "grade": true,
     "grade_id": "cell-657d74b9707831fe",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ij8iQGpMvRrX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977c913c40b5c7ca9ab33f6b41b6e6a2",
     "grade": false,
     "grade_id": "cell-5b88cc469821dbe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let us train the NN model we previously created.\n",
    "\n",
    "First, let us implement the `evaluate` function that will be called to evaluate the model performance when training.\n",
    "\n",
    "***Note:*** Our evaluation uses the same loss function that we use during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.916940Z",
     "start_time": "2023-02-28T02:57:20.501045Z"
    },
    "deletable": false,
    "id": "EOjEFit_vRrX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "#input: Y_pred,Y_true\n",
    "#output: mean squared error, mean absolute error\n",
    "def classification_metrics(X_reconstructed, X_original):\n",
    "    mse, mae = mean_squared_error(X_original, X_reconstructed), \\\n",
    "               mean_absolute_error(X_original, X_reconstructed)\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "\n",
    "#input: model, loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_X_original = torch.FloatTensor()\n",
    "    all_X_reconstructed = torch.FloatTensor()\n",
    "    for x, _ in loader:\n",
    "        x_reconstructed = model(x)\n",
    "        \"\"\"\n",
    "        TODO: Add the correct values to the lists in order to keep a\n",
    "        running tab of all of the original and reconstructed inputs.\n",
    "        \n",
    "        Hint: use torch.cat().\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        all_X_original = torch.cat((all_X_original, x))\n",
    "        all_X_reconstructed = torch.cat((all_X_reconstructed, x_reconstructed))\n",
    "        \n",
    "    mse, mae = classification_metrics(all_X_reconstructed.detach().numpy(), all_X_original.detach().numpy())\n",
    "    print(f\"mse: {mse:.3f}, mae: {mae:.3f}\")\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.965590Z",
     "start_time": "2023-02-28T02:57:20.918604Z"
    },
    "deletable": false,
    "editable": false,
    "id": "L9HTWrsMvRrX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08e13f134270ece398235bbfbe59f555",
     "grade": false,
     "grade_id": "cell-8970a0bdbd7b14bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d31e0567-e8eb-4502-df19-c9e1317358ed"
   },
   "outputs": [],
   "source": [
    "print(\"model perfomance before training:\")\n",
    "# initialized the model\n",
    "model = VanillaAutoencoder()\n",
    "mae_train_init = evaluate(model, train_loader)[1]\n",
    "mae_val_init = evaluate(model, val_loader)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.971059Z",
     "start_time": "2023-02-28T02:57:36.968156Z"
    },
    "deletable": false,
    "editable": false,
    "id": "PmtbM4xLvRrY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91036e646c1a8709c0a733abca3ec69a",
     "grade": false,
     "grade_id": "cell-fa0511ee9c0bbe4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "assert mae_train_init > 0.1, \"mae is less than 0.1! Please check this is random initialization and not training as the performance should be worse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7CKGmm3yvRrW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b6fefce55ffd830402daa4015f5d651",
     "grade": false,
     "grade_id": "cell-a288a935651be8ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This time we will be using a slightly more advanced optimizer option than the SGD optimizer that we have seen in the past. Instead, we will be using the Adam optimize which utilizes concepts such as momentum to offer a more refined and effective training. However, from your end it works almost exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.976335Z",
     "start_time": "2023-02-28T02:57:36.973144Z"
    },
    "deletable": false,
    "id": "Rq5Ov7PsvRrW"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Define the optimizer (Adam) with learning rate 0.001, assign it to `optimizer`.\n",
    "\n",
    "REFERENCE: https://pytorch.org/docs/stable/optim.html\n",
    "\"\"\"\n",
    "def get_optimizer(model):\n",
    "    optimizer = None\n",
    "\n",
    "    # your code here\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "   \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.990531Z",
     "start_time": "2023-02-28T02:57:36.983905Z"
    },
    "deletable": false,
    "editable": false,
    "id": "aExSckTQvRrW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "991adcb67d4ce230275ad74e84ddc2b2",
     "grade": true,
     "grade_id": "cell-5d7a20ac55509d77",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Tpej4gBEvRrY",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4410d25bdddf63ffc10a998cfb2f80a1",
     "grade": false,
     "grade_id": "cell-3f7737daea843131",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To train the model, you should follow the following step:\n",
    "- Clear the gradients of all optimized variables\n",
    "- Forward pass: compute predicted outputs by passing inputs to the model\n",
    "- Calculate the loss (with an extra regularization term if the model is a SparseAutoencoder)\n",
    "- Backward pass: compute gradient of the loss with respect to model parameters\n",
    "- Perform a single optimization step (parameter update)\n",
    "- Update average training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.999120Z",
     "start_time": "2023-02-28T02:57:36.992353Z"
    },
    "deletable": false,
    "id": "TJKLcG1svRrY",
    "outputId": "5163b996-446e-4e5a-e23f-9d4845230747"
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 10\n",
    "    \n",
    "    # get the correct type of optimizer for the model\n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # prep model for training\n",
    "    model.train()\n",
    "\n",
    "    train_loss_arr = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" \n",
    "            TODO: Step 2. perform forward pass using `model`, save the output to x_reconstructed;\n",
    "                  Step 3. calculate the loss using `criterion`, save the output to loss.\n",
    "                      If the model is a SparseAutoencoder, the loss will have an additional\n",
    "                      regularization penalty. This is calculated by:\n",
    "                          average of (- rho * log(data_rho)  +  (1 - rho) * log(1 - data_rho))\n",
    "                      where we will use rho of 0.1\n",
    "            \"\"\"\n",
    "            \n",
    "            x_reconstructed = model.forward(x)\n",
    "            loss = criterion(x_reconstructed, x)\n",
    "            # your code here\n",
    "            \n",
    "            if isinstance(model, SparseAutoencoder):\n",
    "                penalty = None\n",
    "                rho = 0.1\n",
    "                data_rho = model.data_rho\n",
    "                penalty = -(rho * torch.log(data_rho)  +  (1 - rho) * torch.log(1 - data_rho)).mean()\n",
    "                loss = loss + (0.5 * penalty)\n",
    "            \"\"\" Step 4. backward pass \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        if epoch % 2 == 0:\n",
    "            train_loss_arr.append(np.mean(train_loss))\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "            evaluate(model, val_loader)\n",
    "            \n",
    "    return model, train_loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:59:58.010928Z",
     "start_time": "2023-02-28T02:57:37.000785Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b46516921893f6bcb901824ada47e5f",
     "grade": false,
     "grade_id": "cell-09440f46b9f76e9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vanilla_model = VanillaAutoencoder()\n",
    "vanilla_model, vanilla_train_loss_arr = train_model(vanilla_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:02:00.856833Z",
     "start_time": "2023-02-28T02:59:58.012335Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5feff9e0b0d6accf7ac9d3a0c966918",
     "grade": false,
     "grade_id": "cell-c3305e09c12ff3d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sparse_model = SparseAutoencoder()\n",
    "sparse_model, sparse_train_loss_arr = train_model(sparse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:04:16.248451Z",
     "start_time": "2023-02-28T03:02:00.858105Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3701136cc45abf4a32af7ea1b4ba898d",
     "grade": false,
     "grade_id": "cell-d4b24af37b1e2e39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "denoising_model = DenoisingAutoencoder()\n",
    "denoising_model, denoising_train_loss_arr = train_model(denoising_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:31.256159Z",
     "start_time": "2023-02-28T03:04:16.249609Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "862507eee2ce90717680cc54b5bdf975",
     "grade": false,
     "grade_id": "cell-9e3c3ee59a65657b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "stacked_model = StackedAutoencoder()\n",
    "stacked_model, stacked_train_loss_arr = train_model(stacked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:31.261421Z",
     "start_time": "2023-02-28T03:06:31.257386Z"
    },
    "deletable": false,
    "editable": false,
    "id": "hsYle-e_vRrZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d286c67dea9729349e6ba84a72cdfc2d",
     "grade": false,
     "grade_id": "cell-199ef9f45ea5ce75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert sorted(list(np.round(vanilla_train_loss_arr[:5], 2)), reverse=True) == list(np.round(vanilla_train_loss_arr[:5], 2)) and sorted(list(np.round(sparse_train_loss_arr[:5], 2)), reverse=True) == list(np.round(sparse_train_loss_arr[:5], 2)) and sorted(list(np.round(stacked_train_loss_arr[:5], 2)), reverse=True) == list(np.round(stacked_train_loss_arr[:5], 2)), f\"All training losses should decrease! Please check!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:31.267415Z",
     "start_time": "2023-02-28T03:06:31.263180Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "318a6c4a5bdebef1d9ea5cbee661240d",
     "grade": false,
     "grade_id": "cell-b949c913ecd35f7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert np.mean(sparse_train_loss_arr) > np.mean(vanilla_train_loss_arr), f\"Sparse training losses should be higher than the vanilla loss due to the penalty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:40.796442Z",
     "start_time": "2023-02-28T03:06:31.268881Z"
    },
    "deletable": false,
    "editable": false,
    "id": "YyK3teMZvRrZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72a2396f2f9d345c7ca426c920edd0be",
     "grade": true,
     "grade_id": "cell-a52f94729ecc1271",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:49.346350Z",
     "start_time": "2023-02-28T03:06:40.798589Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "849376069841642e5e654493bc02465b",
     "grade": true,
     "grade_id": "cell-594744a45fc85446",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:57.897629Z",
     "start_time": "2023-02-28T03:06:49.348118Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75c30ff2de131057bc754a7158d1e303",
     "grade": true,
     "grade_id": "cell-95a7171d765af36e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:07:06.422261Z",
     "start_time": "2023-02-28T03:06:57.899507Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec8e18e1913e6931665eacd1bb1ff1b",
     "grade": true,
     "grade_id": "cell-2b07ec46c0ea8b12",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HILSnC7XvRrE",
    "jJ4mvmTavRrL",
    "0lnxAWWnvRrQ",
    "j1MoUmqmvRrU"
   ],
   "name": "HW2_NN_pre.ipynb",
   "provenance": []
  },
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW4_Autoencoder/HW4_Autoencoder.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "524px",
    "left": "1423px",
    "right": "20px",
    "top": "120px",
    "width": "348px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
